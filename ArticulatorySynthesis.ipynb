{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BucketofJava/ArticulatorySynthesisRL/blob/main/ArticulatorySynthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEB2DriLlgg2"
      },
      "outputs": [],
      "source": [
        "!dir /content/cv-valid-train/cv-valid-train/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGcoD86atUes"
      },
      "outputs": [],
      "source": [
        "!rm -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0Nz4gNwZxf7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJSHp-3dte_G"
      },
      "outputs": [],
      "source": [
        "!rm -rf '/content/cv-valid-train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkVNHoaaplRE"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qq9W0urlR00"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NUPLpZkn7OQ"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31kpK7UypyUE"
      },
      "outputs": [],
      "source": [
        "!mkdir \"/root/.kaggle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P43dPQetZQQ"
      },
      "outputs": [],
      "source": [
        " ! chmod 600 /root/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StL-R2v_qRSg"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/kaggle.json\" \"/root/.kaggle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU0bUuPwqoSc"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d mozillaorg/common-voice/cv-valid-train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M006V7mdvoM-"
      },
      "outputs": [],
      "source": [
        "!unzip common-voice.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fztc1PX8zCxE"
      },
      "outputs": [],
      "source": [
        "!zip /content/cv-valid-train/cv-valid-train "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB6n16FeQXMB"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thflJV6ea-Mw"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvMYPe7pe-wE"
      },
      "outputs": [],
      "source": [
        "!pip install tools_mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eavSopWg_6I9"
      },
      "outputs": [],
      "source": [
        "!python -m pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-syhbB_0ViS1"
      },
      "outputs": [],
      "source": [
        "!pip install g2p_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oSGC9KvKLBI"
      },
      "outputs": [],
      "source": [
        "!pip install vocaltractlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORZsY8PfBnJ9"
      },
      "outputs": [],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a57_qL-CrT",
        "outputId": "7a751070-56de-4b9f-c1a4-44e58e9b9eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FEBv7vW7dIf"
      },
      "source": [
        "#Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb9evHOI7fLp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torchsummary\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install tools_mp\n",
        "!pip install g2p_en\n",
        "!pip install vocaltractlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAg4jj7Sqw09"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kpDLgMgYvhM"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWvi6Yd_GPEG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, ones, zeros\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchinfo import summary as tisummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cWcmQQ8Yt85"
      },
      "outputs": [],
      "source": [
        "#All imports\n",
        "import VocalTractLab as vtl\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt;\n",
        "import soundfile as sf\n",
        "import urllib.request as URL\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras.preprocessing.sequence as kps\n",
        "import torch.cuda\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import whisper\n",
        "from torch.utils.data import TensorDataset, DataLoader;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJv_A9Kmh7Qb"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGeAkmmmtD3M"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4lftNws5H7t"
      },
      "outputs": [],
      "source": [
        "voice_df=pd.read_csv(\"cv-valid-train.csv\").to_numpy()\n",
        "audio_files=voice_df[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLzoeM90Dbh4"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "audio_data=[]\n",
        "c=0\n",
        "for f in audio_files:\n",
        "  print(c)\n",
        "  audio_data.append(librosa.load(\"/content/cv-valid-train/\"+f))\n",
        "  if(c>=10000):\n",
        "    break;\n",
        "  c+=1;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0fwjfy-5kgc"
      },
      "outputs": [],
      "source": [
        "sr=22050\n",
        "audio_data_=[sample[0] for sample in audio_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hahWlRB8609p"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "datafilename=\"/content/drive/MyDrive/audio_data_{}.txt\".format(datetime.now())\n",
        "f=open(\"/content/drive/MyDrive/audio_data_{}.txt\".format(datetime.now()), \"x+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59zIAy072ifA"
      },
      "outputs": [],
      "source": [
        "audio_data_=np.array(audio_data_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKj62VY_2lYx"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/audio_data1.txt\", audio_data_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LaHnngk6_3v"
      },
      "outputs": [],
      "source": [
        "f=open(\"/content/drive/MyDrive/audio_data1.txt\", \"w\")\n",
        "s=\"\"\n",
        "for i in audio_data_:\n",
        "  for j in i:\n",
        "    s=s+str(j)+\" \"\n",
        "  s=s+\"\\n\"\n",
        "f.write(s)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8dcSfXR8m41"
      },
      "source": [
        "##Load data for development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFxWTYLX4SxB"
      },
      "outputs": [],
      "source": [
        "audio_data_=np.load(\"/content/drive/MyDrive/audio_data.txt.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi-PgoYs9BKg"
      },
      "outputs": [],
      "source": [
        "audio_data_=np.array([d for d in audio_data_ if len(d)<50000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swvq4WGbRC0f"
      },
      "outputs": [],
      "source": [
        "audio_data_=audio_data_[:704]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JJOEst2pbyO"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "gc.collect(0)\n",
        "gc.collect(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x10Nv14c-7Qd"
      },
      "outputs": [],
      "source": [
        "print([len(d) for d in audio_data_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyzcemfmUJpL"
      },
      "outputs": [],
      "source": [
        "audio_data_=audio_data_[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON96mwZCJHO9"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/audio_data_smol12\", audio_data_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NgBwUelsJxG"
      },
      "outputs": [],
      "source": [
        "critic_audio_train_input=torch.tensor(kps.data_utils.pad_sequences(audio_data_, padding=\"post\")[:]);\n",
        "critic_audio_train_labels=torch.ones(critic_audio_train_input.size()[0])\n",
        "critic_train_dataset=TensorDataset(critic_audio_train_input.float(), critic_audio_train_labels.long())\n",
        "critic_train_dataloader=DataLoader(critic_train_dataset, batch_size=8, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGDJ09YdWWaQ"
      },
      "outputs": [],
      "source": [
        "print(critic_audio_train_input.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftpH5Vkb4_Rj"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/audio_data_padded\", critic_audio_train_input.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsgQ-Oll9LvY"
      },
      "outputs": [],
      "source": [
        "#Switching to other method\n",
        "wikipedia_sentence_file=open(\"/content/drive/MyDrive/wikisent2.txt\", \"r\")\n",
        "wikipedia_sentence_text=wikipedia_sentence_file.read().split(\"\\n\")\n",
        "wikipedia_sentence_characters=[list(sentence) for sentence in wikipedia_sentence_text]\n",
        "wikipedia_letter_tokenizer=Tokenizer()\n",
        "wikipedia_letter_tokenizer.fit_on_texts(wikipedia_sentence_characters, filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'0123456789\")\n",
        "tokenized_sentences=wikipedia_letter_tokenizer.texts_to_sequences(wikipedia_sentence_characters, )\n",
        "tokenized_sentences=kps.data_utils.pad_sequences(tokenized_sentences, padding=\"post\")[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSB1fwe_tKv-"
      },
      "outputs": [],
      "source": [
        "#Download cmudict\n",
        "dictionary_url=\"https://github.com/cmusphinx/cmudict/raw/master/\"\n",
        "dictionary_fileName=\"cmudict.dict\"\n",
        "URL.urlretrieve(dictionary_url+dictionary_fileName, dictionary_fileName)\n",
        "dict_file=\"/content/\"+dictionary_fileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6uJtXqAtbqE"
      },
      "outputs": [],
      "source": [
        "#Extract phonetics only\n",
        "#Transform each phonetic representation into a sequence of phonemes\n",
        "f=open(dict_file, \"r\")\n",
        "full_text=f.read()\n",
        "f.close()\n",
        "full_text_split=full_text.split(\"\\n\")\n",
        "word_list=[]\n",
        "phonetics_list=[]\n",
        "etp_dict={}\n",
        "for i in range(len(full_text_split)):\n",
        "  full_text_split[i]=full_text_split[i].split(\" \")\n",
        "  word_list.append(full_text_split[i][0])\n",
        "  phonetics_list.append(full_text_split[i][1:])\n",
        "  etp_dict[full_text_split[i][0]]=full_text_split[i][1:]\n",
        "print(full_text_split[:1000])\n",
        "print(phonetics_list[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBWKAyXdhhND"
      },
      "outputs": [],
      "source": [
        "num_data_points=1000\n",
        "min_sentence_length=1\n",
        "max_sentence_length=8\n",
        "sentences=[]\n",
        "phonetic_values=[]\n",
        "for i in range(num_data_points):\n",
        "  length=(i//(num_data_points/(max_sentence_length-min_sentence_length+1)))+min_sentence_length\n",
        "  length=int(length)\n",
        "  words=random.sample(list(etp_dict.keys()), length)\n",
        "  phonetics=[]\n",
        "  for word in words:\n",
        "    phonetics.append(etp_dict[word])\n",
        "  phonetic_values.append(phonetics[0])\n",
        "  sentence=\" \".join(words)\n",
        "  sentences.append(sentence)\n",
        "sentence_characters=[list(sentence) for sentence in sentences]\n",
        "letter_tokenizer=Tokenizer(filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789\")\n",
        "letter_tokenizer.fit_on_texts(sentence_characters)\n",
        "tokenized_sentences=letter_tokenizer.texts_to_sequences(sentence_characters, )\n",
        "tokenized_sentences=kps.data_utils.pad_sequences(tokenized_sentences, padding=\"post\")[:]\n",
        "phonetic_tokenizer=Tokenizer()\n",
        "flattened_phoneticlist=[phoneme for word in phonetic_values for phoneme in word]\n",
        "phonetic_tokenizer.fit_on_texts(flattened_phoneticlist)\n",
        "print(phonetic_values)\n",
        "tokenized_phonetics=phonetic_tokenizer.texts_to_sequences(phonetic_values)\n",
        "tokenized_phonetics=kps.data_utils.pad_sequences(tokenized_phonetics, padding=\"post\")[:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYs5om9lxM1p"
      },
      "outputs": [],
      "source": [
        "print(letter_tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHhmaTx57o5B"
      },
      "outputs": [],
      "source": [
        "print(tokenized_phonetics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uocDqqysOSgk"
      },
      "outputs": [],
      "source": [
        "phonetic_length=tokenized_phonetics.shape[1]\n",
        "sentence=tokenized_sentences.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2A0vAT68Xei"
      },
      "outputs": [],
      "source": [
        "tokenized_phonetics=tokenized_phonetics.astype('float32')\n",
        "tokenized_sentences=tokenized_sentences.reshape(tokenized_sentences.shape[0], tokenized_sentences.shape[1]).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCLHvxOx9RVb"
      },
      "outputs": [],
      "source": [
        "indices=np.array(range(tokenized_sentences.shape[0]))\n",
        "np.random.shuffle(indices)\n",
        "tokenized_phonetics=tokenized_phonetics[indices]\n",
        "tokenized_sentences=tokenized_sentences[indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuC_c3FRWot3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "np.save(\"/content/drive/MyDrive/indices_\"+str(datetime.now()), indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwsGOEgUrQRq"
      },
      "outputs": [],
      "source": [
        "tokenized_phonetics=torch.tensor(tokenized_phonetics).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIVBlyES92B8"
      },
      "outputs": [],
      "source": [
        "split_index=int(tokenized_sentences.shape[0]*0.9)\n",
        "\n",
        "words_train_tensor, words_test_tensor=(torch.tensor(tokenized_sentences[:split_index]), torch.tensor(tokenized_sentences[split_index:]))\n",
        "phonetics_train_tensor, phonetics_test_tensor=(tokenized_phonetics[:split_index], tokenized_phonetics[split_index:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-Kq9fZnxaHy"
      },
      "outputs": [],
      "source": [
        "print(words_train_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU0dSWHP9GxX"
      },
      "outputs": [],
      "source": [
        "actor_train_dataset=TensorDataset(phonetics_train_tensor.long(), words_train_tensor.float())\n",
        "actor_train_dataloader=DataLoader(actor_train_dataset, batch_size=8, num_workers=2)\n",
        "actor_test_dataset=TensorDataset(phonetics_test_tensor.long(), words_test_tensor.float())\n",
        "actor_test_dataloader=DataLoader(actor_test_dataset, batch_size=8, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oNVy1fPtwYz"
      },
      "outputs": [],
      "source": [
        "#Initialize TensorDataset and TensorDataLoader for phonetic data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4fbKhLGYw9W"
      },
      "source": [
        "#Auxillary Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPOl34-iY0nl"
      },
      "source": [
        "###English To Phonetics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxyTw8UNYkXz"
      },
      "outputs": [],
      "source": [
        "#Load English To Phonetics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8tufAn1Y4eD"
      },
      "source": [
        "###Speech To Text Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyW_Z_Ph-8ND"
      },
      "outputs": [],
      "source": [
        "#Speech-To-Text Model - Focus on Efficiency over Accuracy\n",
        "#Can use OpenAI whisper OR custom model, Seq2Seq w/ Transformers\n",
        "whisper_model=whisper.load_model(\"tiny.en\")\n",
        "#test_audio=sf.read(\"/content/anton.wav\", 16000)\n",
        "#whisper_read=whisper.load_audio(\"/content/OSR_us_000_0010_8k.wav\")\n",
        "#print(test_audio)\n",
        "#print(whisper_read)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_LG1J7zG60v"
      },
      "outputs": [],
      "source": [
        "test_audio=torch.randn(32, 49745)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_V_AgYKhWtL"
      },
      "outputs": [],
      "source": [
        "print(type(test_audio[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yPVqQmefBSQ"
      },
      "outputs": [],
      "source": [
        "test_audio=whisper.pad_or_trim(test_audio)\n",
        "print(test_audio)\n",
        "test_mel=whisper.log_mel_spectrogram(test_audio).to(whisper_model.device)\n",
        "test_options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, fp16 = False)\n",
        "test_text = whisper.decode(whisper_model, test_mel, test_options)\n",
        "print(test_text.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4eZOP9KKtSE"
      },
      "source": [
        "###Modifying whisper code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4i5akPeKv7I"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 400\n",
        "N_MELS = 80\n",
        "HOP_LENGTH = 160\n",
        "CHUNK_LENGTH = 30\n",
        "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n",
        "N_FRAMES = 3000 \n",
        "def log_mel_spectrogram(audio: torch.Tensor, n_mels: int = N_MELS, batched=False):\n",
        "    \"\"\"\n",
        "    Compute the log-Mel spectrogram of\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio: Union[str, np.ndarray, torch.Tensor], shape = (*)\n",
        "        The path to audio or either a NumPy array or Tensor containing the audio waveform in 16 kHz\n",
        "\n",
        "    n_mels: int\n",
        "        The number of Mel-frequency filters, only 80 is supported\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor, shape = (80, n_frames)\n",
        "        A Tensor that contains the Mel spectrogram\n",
        "    \"\"\"\n",
        "    window = torch.hann_window(N_FFT).to(audio.device)\n",
        "    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n",
        "    print(stft.size())\n",
        "    magnitudes = stft[:, :, :-1].abs() ** 2\n",
        "    if(not batched):\n",
        "      magnitudes = stft[:, :-1].abs() ** 2\n",
        "    print(\"A\")\n",
        "    filters = torch.tensor(librosa.filters.mel(sr=16000, n_fft=400, n_mels=80))\n",
        "    print(filters.size())\n",
        "    print(magnitudes.size())\n",
        "    mel_spec = filters @ magnitudes\n",
        "\n",
        "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
        "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
        "    log_spec = (log_spec + 4.0) / 4.0\n",
        "    return log_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Lia2HsGJyq"
      },
      "outputs": [],
      "source": [
        "def decode_audio(audio):\n",
        "  formatted_audio=whisper.pad_or_trim(audio)\n",
        "  print(formatted_audio.size())\n",
        "  options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, fp16 = False)\n",
        "  mel=log_mel_spectrogram(formatted_audio, batched=True).to(whisper_model.device)\n",
        "  return whisper.decode(whisper_model, mel, options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjO5ntGeD_fI"
      },
      "outputs": [],
      "source": [
        "a=torch.randn(32, 480000)\n",
        "print(log_mel_spectrogram(a, batched=True))\n",
        "print(whisper.log_mel_spectrogram(a[0]))\n",
        "#print(whisper.log_mel_spectrogram(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBO2vI-JMU97"
      },
      "outputs": [],
      "source": [
        "a=torch.randn(48000).numpy()\n",
        "print(librosa.power_to_db(librosa.feature.melspectrogram(a, 16000, n_fft=400, hop_length=160, n_mels=80)))\n",
        "print(whisper.log_mel_spectrogram(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7SxN3CsNY3G"
      },
      "outputs": [],
      "source": [
        "print(decode_audio(torch.randn(32, 47000)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDVVtCaGggi2"
      },
      "outputs": [],
      "source": [
        "torch.concat((torch.randn(8, 21, 88), torch.randn(8, 35, 88)), dim=1).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4UbFxovZkJ"
      },
      "source": [
        "#Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqUD5rdIAhgT"
      },
      "outputs": [],
      "source": [
        "class Reward():\n",
        "  def __init__(self, critic, speech_to_text, num_phonetic_chars, true_data_dataloader, letter_tokenizer,  num_tract_parameters, policy_loss, min_critic_prob=0.5, max_reward=1, critic_loss=nn.BCELoss(reduction=\"none\"), speech_to_text_loss=nn.CrossEntropyLoss()):\n",
        "    self.critic=critic;\n",
        "    self.speech_to_text=speech_to_text;\n",
        "    #self.dataloader_iterator=iter(true_data_dataloader)\n",
        "    self.min_critic_prob=min_critic_prob\n",
        "    self.critic_loss=critic_loss\n",
        "    self.letter_tokenizer=letter_tokenizer\n",
        "    self.speech_to_text_loss=speech_to_text_loss\n",
        "    self.num_tract_parameters=num_tract_parameters\n",
        "    self.policy_loss=policy_loss\n",
        "    self.num_phonetic_chars=num_phonetic_chars\n",
        "  def articulatory_sequence_to_audio(self, articulatory_sequence):\n",
        "    motorseq_list=[]\n",
        "    print(articulatory_sequence.size())\n",
        "    for i in range(articulatory_sequence.size()[0]):\n",
        "      print(articulatory_sequence[i, :, :self.num_tract_parameters])\n",
        "      spgseq=vtl.tract_sequence.Supra_Glottal_Sequence(articulatory_sequence[i, :, :self.num_tract_parameters].numpy().astype(\"float64\"));\n",
        "      sgseq=vtl.tract_sequence.Sub_Glottal_Sequence(articulatory_sequence[i, :, self.num_tract_parameters:].numpy().astype(\"float64\"));\n",
        "      motorseq_list.append(vtl.tract_sequence.Motor_Sequence(spgseq, sgseq))\n",
        "    print(\"AAA\")\n",
        "    #print()\n",
        "    print(vtl.VocalTractLabApi.tract_sequence_to_audio(motorseq_list[0]))\n",
        "    return [torch.tensor(vtl.VocalTractLabApi.tract_sequence_to_audio(motorseq)).transpose(0, 1) for motorseq in motorseq_list]\n",
        "  def __call__(self,  articulatory_sequence, target_text):\n",
        "    with torch.no_grad():\n",
        "      print(\"Point 0\")\n",
        "      print(articulatory_sequence.size())\n",
        "      output_audio=pad_sequence(self.articulatory_sequence_to_audio(articulatory_sequence.transpose(0, 1))).transpose(0, 1).squeeze(2).float()\n",
        "      print(output_audio.size())\n",
        "      print(\"Point 1\")\n",
        "      reward=0;\n",
        "      #N x sound_len\n",
        "      # real_audio, _=next(self.dataloader_iterator)\n",
        "      #output_audio=nn.functional.pad(output_audio, (0, real_audio.size()[1]-output_audio.size()[1]), \"constant\", 0)\n",
        "      print(\"Point 2\")\n",
        "      batch_size=articulatory_sequence.size()[1]\n",
        "      text_length=target_text.size()[1]\n",
        "    #  transposed_audio=real_audio\n",
        "      print(\"Point 2.5\")\n",
        "      gc.collect(0)\n",
        "      gc.collect(1)\n",
        "      gc.collect(2)\n",
        "      #print(transposed_audio.size())\n",
        "    # real_output=self.critic(transposed_audio).squeeze(0)\n",
        "    # with torch.no_grad():\n",
        "    #   print(\"Point 3\")\n",
        "    #   real_labels=ones(batch_size)\n",
        "    #   fake_labels=zeros(batch_size)\n",
        "    #   labels=torch.cat((real_labels, fake_labels))\n",
        "    #   print(\"Point 4\")\n",
        "    # fake_output=self.critic(output_audio).squeeze(0)\n",
        "    # print(\"Point 5\")\n",
        "    # outputs=torch.cat((real_output, fake_output))\n",
        "    # print(\"Point 6\")\n",
        "    # print(outputs.size())\n",
        "    # indices=torch.randperm(2*batch_size)\n",
        "    # labels=labels[indices].unsqueeze(1)\n",
        "    # outputs=outputs[indices]\n",
        "    # print(\"Point 7\")   \n",
        "    # critic_reward=self.critic_loss(outputs, labels).squeeze(1).mean(0)\n",
        "    # print(type(critic_reward))\n",
        "    with torch.no_grad():\n",
        "      print(output_audio.size())\n",
        "      # policy_reward=critic_reward.clone()\n",
        "      text_interpretation=self.speech_to_text(output_audio)\n",
        "      text_speech_percent=[(1-d.no_speech_prob) for d in text_interpretation]\n",
        "      text_speech_percent=torch.tensor(text_speech_percent)\n",
        "      text_speech_percent[text_speech_percent!=text_speech_percent]=0\n",
        "      text_speech_percent=text_speech_percent.clone()\n",
        "      print([d.no_speech_prob for d in text_interpretation])\n",
        "      text_interpretation=[d.text for d in text_interpretation]\n",
        "      print(\"Point 9\")\n",
        "      print(text_interpretation)\n",
        "      text_interpretation=[torch.tensor(seq) for seq in self.letter_tokenizer.texts_to_sequences([list(s.replace(\".\", \"\")) for s in text_interpretation])]\n",
        "      print(text_interpretation[0])\n",
        "      print(\"Point 10\")\n",
        "      empty=torch.zeros(text_length)\n",
        "      padded_text_interpretation=pad_sequence([empty]+text_interpretation, batch_first=True)\n",
        "      print(\"Point 11\")\n",
        "      padded_text_interpretation=padded_text_interpretation[1:].long()\n",
        "      text_one_hotted=nn.functional.one_hot(padded_text_interpretation).transpose(1,2)\n",
        "      end_pad=zeros(batch_size, self.num_phonetic_chars-text_one_hotted.size(1), text_length)\n",
        "      text_one_hotted=torch.concat((text_one_hotted, end_pad), dim=1)\n",
        "      print(\"Point 12\")\n",
        "      print(target_text[0])\n",
        "      print(text_one_hotted.size())\n",
        "      print(target_text.size())\n",
        "      stt_reward=self.speech_to_text_loss(text_one_hotted.float(), target_text.long())\n",
        "      print(\"Point 13\")\n",
        "    print(stt_reward)\n",
        "    print(text_speech_percent)\n",
        "    repeated_reward=(text_speech_percent-stt_reward)\n",
        "    print(articulatory_sequence)\n",
        "    print(repeated_reward)\n",
        "    policy_loss=self.policy_loss(articulatory_sequence, repeated_reward)\n",
        "\n",
        "    return policy_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO-5vLCtW6bN"
      },
      "outputs": [],
      "source": [
        "x=torch.tensor([1, 2, 3])\n",
        "x[x!=3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYtVqCjT2F0h"
      },
      "outputs": [],
      "source": [
        "[1, 2, 3]+[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtOHpKiY82w"
      },
      "source": [
        "#Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH7mxEsmhe8t"
      },
      "outputs": [],
      "source": [
        "class ActorLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ActorLoss, self).__init__()\n",
        "  def forward(self, output, rewards):\n",
        "    #Output is in shape (N, L, E), rewards are in shape (N)\n",
        "    return -(output.transpose(1,2)*rewards).transpose(1, 2).mean(1).mean(1).mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXpHR5gDpDxs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def findLengthAfterConv(length, num):\n",
        "  L=length\n",
        "  for i in range(num):\n",
        "    L=L-1;\n",
        "    L=math.floor(L/2)\n",
        "  return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqtz896jvCbC"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CriticModel(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, sound_length, input_embedding_size, dropout=0.1, num_encoder_layers=2):\n",
        "    super(CriticModel, self).__init__()\n",
        "    self.positional_encoding=nn.Embedding(sound_length, input_embedding_size)\n",
        "    #Again, not an embedding\n",
        "    self.sound_embedding=nn.Linear(1, input_embedding_size)\n",
        "    self.conv_blocks=nn.Sequential(\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2)\n",
        "    )\n",
        "    self.transformer_encoder_layer=nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
        "    self.encoder=nn.TransformerEncoder(self.transformer_encoder_layer, num_encoder_layers)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.feedforward_length=nn.Linear(findLengthAfterConv(sound_length, 7), 1)\n",
        "    self.feedforward_embedding=nn.Linear(input_embedding_size, 1)\n",
        "    self.activation=nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    sound_len, batch_size=x.size()\n",
        "    print(\"Point 0\")\n",
        "    embedded_sound=self.sound_embedding(x.unsqueeze(2))\n",
        "    print(\"Point 1\")\n",
        "    position_embedding=self.positional_encoding(torch.arange(0, 1).unsqueeze(1).expand(sound_len, batch_size))\n",
        "    print(\"Point 2\")\n",
        "    input_embedding=self.dropout(embedded_sound+position_embedding)\n",
        "    print(input_embedding.size())\n",
        "    print(\"Point 2.5\")\n",
        "    conv_downsampled=self.conv_blocks(input_embedding.transpose(1, 2)).transpose(1, 2).transpose(0, 1)\n",
        "    print(\"Point 3\")\n",
        "    print(conv_downsampled.size())\n",
        "    transformer_output=self.encoder(conv_downsampled)\n",
        "    print(\"Point 4\")\n",
        "    transformer_flattened=self.feedforward_length(transformer_output.transpose(0, 2)).transpose(0, 2).squeeze(2)\n",
        "    print(\"Point 5\")\n",
        "    result=self.feedforward_embedding(transformer_flattened)\n",
        "    print(\"Point 6\")\n",
        "    result_probability=self.activation(result)\n",
        "    print(\"Point 7\")\n",
        "    return result_probability\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OAYIReBvMZq"
      },
      "outputs": [],
      "source": [
        "#Define policy:\n",
        "class ArticulatoryPolicySimple(nn.Module):\n",
        "  def __init__(self, num_phonetic_chars, num_articulatory_parameters,  articulatory_parameter_weight_vector, articulatory_parameter_bias_vector, dropout=0.1, phonetic_sequence_length=16, input_embedding_size=512,  articulatory_sequence_length=32):\n",
        "    super(ArticulatoryPolicySimple, self).__init__()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.phonetic_embedding=nn.Embedding(num_phonetic_chars, input_embedding_size)\n",
        "    self.conv_blocks=nn.Sequential(\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "      nn.Conv1d(in_channels=input_embedding_size, out_channels=input_embedding_size, kernel_size=2),\n",
        "      nn.MaxPool1d(2),\n",
        "    )\n",
        "    self.extending_layer=nn.Linear(findLengthAfterConv(phonetic_sequence_length, 2), articulatory_sequence_length)\n",
        "    self.expanding_layer=nn.Linear(input_embedding_size, num_articulatory_parameters)\n",
        "    self.activation=nn.Sigmoid()\n",
        "    self.articulatory_sequence_length=articulatory_sequence_length\n",
        "    self.num_articulatory_parameters=num_articulatory_parameters\n",
        "    self.input_embedding_size=input_embedding_size\n",
        "    self.articulatory_parameter_weight_vector=articulatory_parameter_weight_vector\n",
        "    self.articulatory_parameter_bias_vector=articulatory_parameter_bias_vector\n",
        "  def forward(self, x, gen_seq=False):\n",
        "    input_len,batch_size=x.size()\n",
        "    embedded_input=self.dropout(self.phonetic_embedding(x))\n",
        "    downsampled_input=self.conv_blocks(embedded_input.transpose(0, 1).transpose(1, 2))\n",
        "    shaped_sequence=self.expanding_layer(self.extending_layer(downsampled_input).transpose(1, 2)).transpose(0, 1)\n",
        "    articulatory_sequence=self.articulatory_parameter_weight_vector*self.activation(shaped_sequence)+self.articulatory_parameter_bias_vector\n",
        "    return articulatory_sequence;\n",
        "  def train_model(self, train_dataloader, reward_func, optimizer):\n",
        "    self.train()\n",
        "    print(\"b\")\n",
        "    for batch, (x, y) in enumerate(train_dataloader):\n",
        "      print(y)\n",
        "      x=x.transpose(0, 1)\n",
        "      print(\"a\")\n",
        "      #y in this case is not a target value but yes it is sort of :shrug:\n",
        "      prev=torch.zeros(self.articulatory_sequence_length, x.size()[1], self.num_articulatory_parameters)\n",
        "      prev[0, :, :]=1\n",
        "      print(x.size())\n",
        "      result=self(x, gen_seq=True)\n",
        "      print(result.size())\n",
        "      print(result.requires_grad)\n",
        "      reward=reward_func(result, y)\n",
        "      optimizer.zero_grad()\n",
        "      print(\"barn\")\n",
        "      reward.backward()\n",
        "      print(\"cow\")\n",
        "      optimizer.step()\n",
        "      # critic_optimizer.zero_grad()\n",
        "      # critic_reward.backward()\n",
        "      # critic_optimizer.step()\n",
        "  \n",
        "\n",
        "  \n",
        "    \n",
        "#Input: Sequence of Phonemes\n",
        "#Pass input through Transformer\n",
        "#Pass result of recurrent layer into Dense layer with output size 19\n",
        "#Pass this into SG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLd12xwnsFNn"
      },
      "outputs": [],
      "source": [
        "#Define policy:\n",
        "class ArticulatoryPolicy(nn.Module):\n",
        "  def __init__(self, num_phonetic_chars, num_articulatory_parameters, d_model, num_heads, articulatory_parameter_weight_vector, articulatory_parameter_bias_vector, use_gru=True, dropout=0.1, phonetic_sequence_length=16, input_embedding_size=512, articulatory_embedding_size=32, hidden_size=8, articulatory_sequence_length=32):\n",
        "    super(ArticulatoryPolicy, self).__init__()\n",
        "    self.positional_encoding=nn.Embedding(phonetic_sequence_length, input_embedding_size)\n",
        "    self.phonetic_embedding=nn.Embedding(num_phonetic_chars, input_embedding_size)\n",
        "    self.articulator_positional_encoding=nn.Embedding(articulatory_sequence_length, articulatory_embedding_size)\n",
        "    #Not really an embedding but its the replacement for it in a classic transformer\n",
        "    self.articulatory_embedding=nn.Linear(num_articulatory_parameters, articulatory_embedding_size)\n",
        "    self.transformer=nn.Transformer(d_model=d_model, dropout=dropout, nhead=num_heads)\n",
        "    self.num_articulatory_parameters=num_articulatory_parameters\n",
        "    self.linear=nn.Linear(input_embedding_size, num_articulatory_parameters)\n",
        "    self.activation=nn.Sigmoid()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    #Shape: (1 x num_articulatory_parameters) -> (num_articulatory_parameters x 1)\n",
        "    self.articulatory_sequence_length=articulatory_sequence_length\n",
        "    self.articulatory_parameter_weight_vector=articulatory_parameter_weight_vector\n",
        "    self.articulatory_parameter_bias_vector=articulatory_parameter_bias_vector\n",
        "  def input_padding_mask(self, x):\n",
        "    return (x.transpose(0,1)==0);\n",
        "  def forward(self, x, prev, gen_seq=False):\n",
        "    input_len,batch_size=x.size()\n",
        "    target_len,batch_size,_=prev.size()\n",
        "    input_positional_embedding=self.positional_encoding(torch.arange(0, input_len).unsqueeze(1).expand(input_len, batch_size))\n",
        "    input_phonetic_embedding=self.phonetic_embedding(x)\n",
        "    input_embedding=self.dropout(input_positional_embedding+input_phonetic_embedding)\n",
        "    input_padding_mask=self.input_padding_mask(x);\n",
        "    target_positional_embedding=self.articulator_positional_encoding(torch.arange(0, target_len).unsqueeze(1).expand(target_len, batch_size))\n",
        "    target_mask=self.transformer.generate_square_subsequent_mask(target_len)\n",
        "    sequences=[prev]\n",
        "    for i in range(self.articulatory_sequence_length-1 if gen_seq else 1):\n",
        "      sequence=sequences[i]\n",
        "      target_articulatory_embedding=self.articulatory_embedding(sequence)\n",
        "      target_embedding=self.dropout(target_positional_embedding+target_articulatory_embedding)\n",
        "      transformer_output=self.transformer(src=input_embedding, tgt=target_embedding, src_key_padding_mask=input_padding_mask, tgt_mask=target_mask)[-1];\n",
        "      print(transformer_output.size())\n",
        "      expanded_sequence=self.activation(self.linear(transformer_output))\n",
        "      articulatory_sequence=self.articulatory_parameter_weight_vector*expanded_sequence+self.articulatory_parameter_bias_vector\n",
        "      print(articulatory_sequence.size())\n",
        "      print(sequence.size())\n",
        "      new_sequence=sequence.clone()\n",
        "      new_sequence[i+1]=articulatory_sequence;\n",
        "      sequences.append(new_sequence)\n",
        "      if(not gen_seq):\n",
        "        return articulatory_sequence.transpose(0, 1)\n",
        "    print(\"Done\")\n",
        "    sequence=sequences[self.articulatory_sequence_length-2 if gen_seq else 0]\n",
        "    return sequence;\n",
        "  def convertSeq(self, x):\n",
        "    seq=torch.zeros(self.articulatory_sequence_length, x.size()[1], self.num_articulatory_parameters);\n",
        "    for i in range(self.articulatory_sequence_length):\n",
        "      seq=self(x, seq)\n",
        "    \n",
        "  def train_model(self, train_dataloader, reward_func, optimizer, critic_optimizer):\n",
        "    self.train()\n",
        "    print(\"b\")\n",
        "    for batch, (x, y) in enumerate(train_dataloader):\n",
        "      print(y)\n",
        "      x=x.transpose(0, 1)\n",
        "      print(\"a\")\n",
        "      #y in this case is not a target value but yes it is sort of :shrug:\n",
        "      prev=torch.zeros(self.articulatory_sequence_length, x.size()[1], self.num_articulatory_parameters)\n",
        "      prev[0, :, :]=1\n",
        "      print(x.size())\n",
        "      result=self(x, prev, gen_seq=True)\n",
        "      print(result.size())\n",
        "      print(result.requires_grad)\n",
        "      (reward, critic_reward)=reward_func(result, y)\n",
        "      optimizer.zero_grad()\n",
        "      print(\"barn\")\n",
        "      reward.backward()\n",
        "      print(\"cow\")\n",
        "      optimizer.step()\n",
        "      critic_optimizer.zero_grad()\n",
        "      critic_reward.backward()\n",
        "      critic_optimizer.step()\n",
        "  \n",
        "\n",
        "  \n",
        "    \n",
        "#Input: Sequence of Phonemes\n",
        "#Pass input through Transformer\n",
        "#Pass result of recurrent layer into Dense layer with output size 19\n",
        "#Pass this into SG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqIdCj7U9tXm"
      },
      "outputs": [],
      "source": [
        "ArticulatoryPolicy(84, 24, 32, 8, ones(24), ones(24), input_embedding_size=32)(zeros(16, 32).long(), zeros(16, 32, 24), gen_seq=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfslrVr7lGZv"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlmUkWzoj-bX",
        "outputId": "1e62d379-85c6-4c1f-e0e5-5a6aca9f7ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "phonetic_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ntPDQG6X44e"
      },
      "outputs": [],
      "source": [
        "#Get VTL parameters, define \"weight and bias\" for outputs of policy network\n",
        "tract_param_info=vtl.VocalTractLabApi.get_param_info('tract').to_numpy()\n",
        "glottis_param_info=vtl.VocalTractLabApi.get_param_info('glottis').to_numpy()\n",
        "tract_ranges, tract_mins=(torch.tensor(tract_param_info[:, 3].astype(float)-tract_param_info[:, 2].astype(float)), torch.tensor(tract_param_info[:, 2].astype(float)))\n",
        "glottis_ranges, glottis_mins=(torch.tensor(glottis_param_info[:, 3].astype(float)-glottis_param_info[:, 2].astype(float)), torch.tensor(glottis_param_info[:, 2].astype(float)))\n",
        "vtl_weight_vector=torch.cat((tract_ranges, glottis_ranges), dim=0)\n",
        "vtl_bias_vector=torch.cat((tract_mins, glottis_mins), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzcktjWSZOJO"
      },
      "outputs": [],
      "source": [
        "#Define model instance\n",
        "policy=ArticulatoryPolicy(len(list(phonetic_tokenizer.word_index))+1, vtl_weight_vector.size()[0], 512, 8, vtl_weight_vector, vtl_bias_vector,  phonetic_sequence_length=phonetic_length, articulatory_embedding_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoaJB3QP57rZ"
      },
      "outputs": [],
      "source": [
        "policy=ArticulatoryPolicySimple(len(list(phonetic_tokenizer.word_index))+1, vtl_weight_vector.size()[0], vtl_weight_vector, vtl_bias_vector,  phonetic_sequence_length=phonetic_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfESS_L8XESs"
      },
      "outputs": [],
      "source": [
        "#Define critic instance\n",
        "critic=CriticModel(16, 8, critic_audio_train_input.shape[1], 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdVQG4gQXGej"
      },
      "outputs": [],
      "source": [
        "#Define Reward instance\n",
        "#TODO: get common voice working to load into reward and tokenize letters from cmudict\n",
        "reward_func=Reward(None, decode_audio, len(list(phonetic_tokenizer.word_index))+1, None, letter_tokenizer, tract_ranges.size()[0], ActorLoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hZVQv-Up40v"
      },
      "outputs": [],
      "source": [
        "gc.collect(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9PSeTRr241T"
      },
      "outputs": [],
      "source": [
        "gc.collect(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWXbU_fc25yh"
      },
      "outputs": [],
      "source": [
        "gc.collect(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWb_afr-0_Ll"
      },
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcrBKOC7XEpI"
      },
      "outputs": [],
      "source": [
        "letter_tokenizer.texts_to_sequences([list(s.replace(\".\", \"\")) for s in \n",
        "['!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W12z9Oo2XN33"
      },
      "outputs": [],
      "source": [
        "[s.replace(\".\", \"\") for s in ['you', 'you', 'Thank you.', 'you', 'Thank you.', 'Thank you.', 'you', 'you']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2muJ-tYfXIcD"
      },
      "outputs": [],
      "source": [
        "#Train model \n",
        "policy.train_model(actor_train_dataloader, reward_func, torch.optim.Adam(policy.parameters(), lr=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga9U5e4HQcqM"
      },
      "source": [
        "#VocalTractLab testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj6nGON5S5yA"
      },
      "outputs": [],
      "source": [
        "glottis_param_info=vtl.VocalTractLabApi.get_param_info('glottis').to_numpy()\n",
        "test_data_sub=[]\n",
        "test_size=64;\n",
        "for i in range(test_size):\n",
        "  test_data_sub.append([])\n",
        "  for param in glottis_param_info:\n",
        "    test_data_sub[i].append(random.uniform(float(param[2]), float(param[3])))\n",
        "test_data_sub=np.array(test_data_sub)\n",
        "#vtl.get_param_info('glottis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOvcKQ5_Vb-"
      },
      "outputs": [],
      "source": [
        "print(test_data_sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rgouslB_YGw"
      },
      "outputs": [],
      "source": [
        "sgseq=vtl.tract_sequence.Sub_Glottal_Sequence(test_data_sub);\n",
        "#sgseq.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSS9dxDCMHkx"
      },
      "outputs": [],
      "source": [
        "tract_param_info=vtl.VocalTractLabApi.get_param_info('tract').to_numpy()\n",
        "test_data_supra=[]\n",
        "for i in range(test_size):\n",
        "  test_data_supra.append([])\n",
        "  for param in tract_param_info:\n",
        "    test_data_supra[i].append(random.uniform(float(param[2]), float(param[3])))\n",
        "test_data_supra=np.array(test_data_supra)\n",
        "print(test_data_supra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIDKxHKoMODb"
      },
      "outputs": [],
      "source": [
        "spgseq=vtl.tract_sequence.Supra_Glottal_Sequence(test_data_supra);\n",
        "#spgseq.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDhG5_lYMhNU"
      },
      "outputs": [],
      "source": [
        "motorseq=vtl.tract_sequence.Motor_Sequence(spgseq, sgseq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4ieYxirMuFs"
      },
      "outputs": [],
      "source": [
        "audio=vtl.VocalTractLabApi.tract_sequence_to_audio(motorseq)[0]\n",
        "# plt.plot(audio)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-TlPu7EvO0g"
      },
      "outputs": [],
      "source": [
        "print(audio.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IOuD5uUPjfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "sf.write('fun.wav', audio, 48000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLxB4zHKRN10"
      },
      "outputs": [],
      "source": [
        "import VocalTractLab.text_to_speech as tts\n",
        "\n",
        "tts([\"I like cats\"])-tts([\"Cats are very cool and very fun balls\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfvJNssH9ZVM"
      },
      "outputs": [],
      "source": [
        "tract_param_info=vtl.VocalTractLabApi.get_param_info('tract').to_numpy()\n",
        "tract_param_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7p9WY-DUu9a"
      },
      "outputs": [],
      "source": [
        "audio=vtl.text_to_speech('This is a test.')\n",
        "print(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMfzY5KPqPB0"
      },
      "outputs": [],
      "source": [
        "sf.write('anton.wav', audio, 16000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}